{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiklpuerto69\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_PROJECT=LLM_Science_Exam\n"]}],"source":["%env WANDB_PROJECT=LLM_Science_Exam"]},{"cell_type":"code","execution_count":30,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-28T20:17:45.887043Z","iopub.status.busy":"2023-08-28T20:17:45.886034Z","iopub.status.idle":"2023-08-28T20:17:45.938768Z","shell.execute_reply":"2023-08-28T20:17:45.937630Z","shell.execute_reply.started":"2023-08-28T20:17:45.886999Z"},"trusted":true},"outputs":[],"source":["\n","import pandas as pd\n","from datasets import Dataset,DatasetDict\n","from transformers import AutoTokenizer,AutoModelForMultipleChoice, TrainingArguments, Trainer, BitsAndBytesConfig\n","from accelerate import Accelerator\n","import peft\n","from dataclasses import dataclass\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from typing import Optional, Union\n","import torch\n","import datetime\n","import numpy as np\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# Def Functions\n","\n","options = 'ABCDE'\n","indices = list(range(5))\n","\n","option_to_index = {option: index for option, index in zip(options, indices)}\n","index_to_option = {index: option for option, index in zip(options, indices)}\n","\n","def preprocess(example):\n","    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n","    # so we'll copy our question 5 times before tokenizing\n","    first_sentence = [example['prompt']] * 5\n","    second_sentence = []\n","    for option in options:\n","        second_sentence.append(example[option])\n","    # Our tokenizer will turn our text into token IDs BERT can understand\n","    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True) # tokenizer call using 'text_pair' which basically just adds a separator between the two sentences\n","    tokenized_example['label'] = option_to_index[example['answer']]\n","    return tokenized_example\n","# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n","# will dynamically pad our questions at batch-time so we don't have to make every question the length\n","# of our longest question.\n","\n","@dataclass\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n","        labels = [feature.pop(label_name) for feature in features]\n","        batch_size = len(features)\n","        num_choices = len(features[0]['input_ids'])\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])\n","        \n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors='pt',\n","        )\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        return batch\n","def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/mowgli/miniconda3/envs/textgen2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Load in Model and Tokenizer\n","model_path = 'microsoft/deberta-v3-base'\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","\n","model = AutoModelForMultipleChoice.from_pretrained(model_path,\n","                                                    device_map={\"\": current_device})#,\n","                                                    #torch_dtype=torch.float16)\n","# model=peft.prepare_model_for_int8_training(model_path)\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 200/200 [00:00<00:00, 1848.88 examples/s]\n","Map: 100%|██████████| 5997/5997 [00:02<00:00, 2075.10 examples/s]\n","Map: 100%|██████████| 200/200 [00:00<00:00, 1863.85 examples/s]\n"]}],"source":["# Read in Train & Test data.\n","\n","current_device = Accelerator().process_index\n","train_df = pd.read_csv('6000_train_examples.csv')\n","train_df=train_df.dropna()\n","train_ds = Dataset.from_pandas(train_df)\n","train_ds = train_ds.remove_columns(['__index_level_0__'])\n","# tokenized_train_ds = train_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer',\"__index_level_0__\"])\n","\n","val_df = pd.read_csv('train.csv')\n","val_df = val_df.dropna().drop('id',axis=1)\n","val_ds = Dataset.from_pandas(val_df)\n","tokenized_val_ds = val_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","\n","datasets = DatasetDict({\n","    \"train\":train_ds,\n","    \"validation\":val_ds\n","})\n","encoded_datasets = datasets.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","\n","\n","# test_df = pd.read_csv('test.csv')\n","# test_df['answer'] = 'A'\n","# test_ds = Dataset.from_pandas(test_df)\n","# tokenized_test_ds = test_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","        num_rows: 5997\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","        num_rows: 200\n","    })\n","})"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["encoded_datasets"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Load in existing peft\n","# from peft import PeftModel\n","\n","\n","# peft_model = PeftModel.from_pretrained(model, \n","#                                        'finetuned/checkpoint-29500', \n","#                                        is_trainable=True)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["# topk_cpu\" not implemented for 'Half\n","import numpy as np\n","def compute_metrics(eval_predictions):\n","    predictions, label_ids = eval_predictions\n","    top_k_probs, top_k_idxs = torch.topk(torch.tensor(predictions), 3, dim=1)\n","    accuracy = (np.array(top_k_idxs[:, 0]) == label_ids).astype(np.float32).mean().item()\n","    accuracy_2 = (np.array(top_k_idxs[:, 1]) == label_ids).astype(np.float32).mean().item()\n","    accuracy_3 = (np.array(top_k_idxs[:, 2]) == label_ids).astype(np.float32).mean().item()\n","    map3 = accuracy + accuracy_2/2 + accuracy_3/3\n","    return {\"accuracy\": accuracy, 'map@3' : map3}"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T20:18:23.360283Z","iopub.status.busy":"2023-08-28T20:18:23.359910Z","iopub.status.idle":"2023-08-28T20:18:23.367485Z","shell.execute_reply":"2023-08-28T20:18:23.366281Z","shell.execute_reply.started":"2023-08-28T20:18:23.360250Z"},"trusted":true},"outputs":[],"source":["# The arguments here are selected to run quickly; feel free to play with them.\n","checkpoint_dir = 'finetuned'\n","current_time=datetime.datetime.now().strftime(\"%H:%M:%S - %B %m %y\")\n","training_args = TrainingArguments(\n","    output_dir=checkpoint_dir,\n","    evaluation_strategy=\"epoch\",\n","#     save_strategy=\"steps\",\n","#     load_best_model_at_end=True,\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    report_to='wandb',\n","    run_name=f'{model_path}_{current_time}'\n",")\n"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T20:21:03.659061Z","iopub.status.busy":"2023-08-28T20:21:03.658317Z","iopub.status.idle":"2023-08-28T20:21:04.104608Z","shell.execute_reply":"2023-08-28T20:21:04.103444Z","shell.execute_reply.started":"2023-08-28T20:21:03.659023Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model, TaskType\n","lora_config = LoraConfig(\n","    r=64, # Rank\n","    lora_alpha=64,\n","    target_modules=[\"query_proj\", \"value_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    # fan_in_fan_out=True,\n","    task_type=TaskType.SEQ_CLS\n",")\n","peft_model = get_peft_model(model, \n","                            lora_config)"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T20:21:07.287099Z","iopub.status.busy":"2023-08-28T20:21:07.286717Z","iopub.status.idle":"2023-08-28T20:21:07.297912Z","shell.execute_reply":"2023-08-28T20:21:07.296856Z","shell.execute_reply.started":"2023-08-28T20:21:07.287068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 2359296\n","all model parameters: 186782978\n","percentage of trainable model parameters: 1.26%\n"]}],"source":["print(print_number_of_trainable_model_parameters(peft_model))"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T20:21:29.736058Z","iopub.status.busy":"2023-08-28T20:21:29.735686Z","iopub.status.idle":"2023-08-28T20:21:29.749459Z","shell.execute_reply":"2023-08-28T20:21:29.748462Z","shell.execute_reply.started":"2023-08-28T20:21:29.736028Z"},"trusted":true},"outputs":[],"source":["# Generally it's a bad idea to validate on your training set, but because our training set\n","# for this problem is so small we're going to train on all our data.\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=encoded_datasets['train'],\n","    eval_dataset=encoded_datasets['validation'],\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T20:21:30.751516Z","iopub.status.busy":"2023-08-28T20:21:30.750314Z","iopub.status.idle":"2023-08-28T20:21:30.757919Z","shell.execute_reply":"2023-08-28T20:21:30.756458Z","shell.execute_reply.started":"2023-08-28T20:21:30.751472Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache() "]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T20:21:31.087166Z","iopub.status.busy":"2023-08-28T20:21:31.086791Z","iopub.status.idle":"2023-08-28T20:48:59.647311Z","shell.execute_reply":"2023-08-28T20:48:59.646338Z","shell.execute_reply.started":"2023-08-28T20:21:31.087134Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7500/7500 12:23, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Map@3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.520700</td>\n","      <td>1.390370</td>\n","      <td>0.490000</td>\n","      <td>0.632500</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.305200</td>\n","      <td>1.280091</td>\n","      <td>0.495000</td>\n","      <td>0.650833</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.264500</td>\n","      <td>1.246065</td>\n","      <td>0.520000</td>\n","      <td>0.670000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.227200</td>\n","      <td>1.211914</td>\n","      <td>0.525000</td>\n","      <td>0.675000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.181200</td>\n","      <td>1.195124</td>\n","      <td>0.520000</td>\n","      <td>0.679167</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.166600</td>\n","      <td>1.175067</td>\n","      <td>0.515000</td>\n","      <td>0.673333</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.146000</td>\n","      <td>1.165484</td>\n","      <td>0.520000</td>\n","      <td>0.681667</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.125300</td>\n","      <td>1.166053</td>\n","      <td>0.515000</td>\n","      <td>0.680000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.131000</td>\n","      <td>1.161060</td>\n","      <td>0.520000</td>\n","      <td>0.680000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.108700</td>\n","      <td>1.161381</td>\n","      <td>0.525000</td>\n","      <td>0.681667</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=7500, training_loss=1.2134420491536457, metrics={'train_runtime': 743.5908, 'train_samples_per_second': 80.649, 'train_steps_per_second': 10.086, 'total_flos': 9280743063064500.0, 'train_loss': 1.2134420491536457, 'epoch': 10.0})"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["# Training should take about a minute\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T21:03:00.862953Z","iopub.status.busy":"2023-08-28T21:03:00.862190Z","iopub.status.idle":"2023-08-28T21:03:00.867706Z","shell.execute_reply":"2023-08-28T21:03:00.866644Z","shell.execute_reply.started":"2023-08-28T21:03:00.862918Z"},"trusted":true},"outputs":[],"source":["del model,peft_model"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["model.save_pretrained('base_models/full_debertav3/full_debertav3')"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["trainer.save_model('peft_adapters/full_debertav3/')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.save_pretrained('')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
