{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_PROJECT=LLM_Science_Exam\n"]}],"source":["%env WANDB_PROJECT=LLM_Science_Exam"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-28T20:17:45.887043Z","iopub.status.busy":"2023-08-28T20:17:45.886034Z","iopub.status.idle":"2023-08-28T20:17:45.938768Z","shell.execute_reply":"2023-08-28T20:17:45.937630Z","shell.execute_reply.started":"2023-08-28T20:17:45.886999Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/mowgli/miniconda3/envs/textgen2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["\n","import pandas as pd\n","from datasets import Dataset,DatasetDict\n","from transformers import AutoTokenizer,AutoModelForMultipleChoice, TrainingArguments, Trainer, BitsAndBytesConfig\n","from accelerate import Accelerator\n","import peft\n","from dataclasses import dataclass\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from typing import Optional, Union\n","import torch\n","import datetime\n","import numpy as np\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Def Functions\n","\n","options = 'ABCDE'\n","indices = list(range(5))\n","\n","option_to_index = {option: index for option, index in zip(options, indices)}\n","index_to_option = {index: option for option, index in zip(options, indices)}\n","\n","def preprocess(example):\n","    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n","    # so we'll copy our question 5 times before tokenizing\n","    first_sentence = [example['prompt']] * 5\n","    second_sentence = []\n","    for option in options:\n","        second_sentence.append(example[option])\n","    # Our tokenizer will turn our text into token IDs BERT can understand\n","    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True) # tokenizer call using 'text_pair' which basically just adds a separator between the two sentences\n","    tokenized_example['label'] = option_to_index[example['answer']]\n","    return tokenized_example\n","# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n","# will dynamically pad our questions at batch-time so we don't have to make every question the length\n","# of our longest question.\n","\n","@dataclass\n","class DataCollatorForMultipleChoice:\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n","        labels = [feature.pop(label_name) for feature in features]\n","        batch_size = len(features)\n","        num_choices = len(features[0]['input_ids'])\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])\n","        \n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors='pt',\n","        )\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        return batch\n","def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Load in Model and Tokenizer\n","current_device = Accelerator().process_index\n","model_path = 'bert-large-cased'\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","\n","# model = AutoModelForMultipleChoice.from_pretrained(model_path,\n","#                                                     device_map={\"\": current_device})#,\n","                                                    #torch_dtype=torch.float16)\n","# model=peft.prepare_model_for_int8_training(model_path)\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 200/200 [00:00<00:00, 1931.84 examples/s]\n","Map: 100%|██████████| 5997/5997 [00:02<00:00, 2153.76 examples/s]\n","Map: 100%|██████████| 200/200 [00:00<00:00, 1996.76 examples/s]\n","Map: 100%|██████████| 200/200 [00:00<00:00, 2014.43 examples/s]\n"]}],"source":["# Read in Train & Test data.\n","\n","train_df = pd.read_csv('6000_train_examples.csv')\n","train_df=train_df.dropna()\n","train_ds = Dataset.from_pandas(train_df)\n","train_ds = train_ds.remove_columns(['__index_level_0__'])\n","# tokenized_train_ds = train_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer',\"__index_level_0__\"])\n","\n","val_df = pd.read_csv('train.csv')\n","val_df = val_df.dropna().drop('id',axis=1)\n","val_ds = Dataset.from_pandas(val_df)\n","tokenized_val_ds = val_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","\n","datasets = DatasetDict({\n","    \"train\":train_ds,\n","    \"validation\":val_ds\n","})\n","encoded_datasets = datasets.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","\n","\n","test_df = pd.read_csv('test.csv')\n","test_df['answer'] = 'A'\n","test_ds = Dataset.from_pandas(test_df.drop('id',axis=1))\n","tokenized_test_ds = test_ds.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'],\n","    num_rows: 200\n","})"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["test_ds"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n","    num_rows: 200\n","})"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_test_ds"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","test_dataloader = DataLoader(tokenized_test_ds, 10, shuffle=False, collate_fn=DataCollatorForMultipleChoice(tokenizer=tokenizer))"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7fcee8ecacb0>"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["test_dataloader"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at base_models/bert-large-cased/bert-large-cased/ were not used when initializing BertForMultipleChoice: ['bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.17.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.12.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.13.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.20.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.19.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.18.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.19.attention.self.value.lora_B.default.weight', 'classifier.modules_to_save.default.weight', 'bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.20.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.13.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'classifier.modules_to_save.default.bias', 'bert.encoder.layer.22.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.15.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.21.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.16.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.13.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.13.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.12.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.22.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.20.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.23.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.21.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.20.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.15.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'classifier.original_module.weight', 'bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.12.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.23.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.15.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.22.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.14.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.17.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.21.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.18.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.17.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.18.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.14.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.22.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.12.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.17.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.14.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.16.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.19.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.14.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.21.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.18.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.19.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.23.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.16.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'bert.encoder.layer.23.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'bert.encoder.layer.15.attention.self.query.lora_B.default.weight', 'bert.encoder.layer.16.attention.self.query.lora_B.default.weight', 'classifier.original_module.bias', 'bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'bert.encoder.layer.1.attention.self.query.lora_B.default.weight']\n","- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMultipleChoice were not initialized from the model checkpoint at base_models/bert-large-cased/bert-large-cased/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]}],"source":["model = AutoModelForMultipleChoice.from_pretrained(f'base_models/bert-large-cased/bert-large-cased/',device_map={\"\": current_device})\n","model = peft.PeftModel.from_pretrained(model, \n","                                       'peft_adapters/bert-large-cased/', \n","                                       is_trainable=False)\n","model.eval()\n","preds = []\n","for batch in test_dataloader:\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    preds.append(outputs.logits.cpu().detach())\n","\n","bert_large_preds = torch.cat(preds)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at base_models/full_debertav3/full_debertav3/ were not used when initializing DebertaV2ForMultipleChoice: ['deberta.encoder.layer.8.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.4.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.2.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.10.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.2.attention.self.query_proj.lora_A.default.weight', 'classifier.original_module.weight', 'deberta.encoder.layer.3.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.5.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.6.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.1.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.6.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.6.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.9.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.4.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.8.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.9.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.3.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.11.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.3.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.8.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.11.attention.self.query_proj.lora_A.default.weight', 'classifier.modules_to_save.default.weight', 'deberta.encoder.layer.5.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.10.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.7.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.7.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.2.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.7.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.1.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.8.attention.self.query_proj.lora_A.default.weight', 'classifier.modules_to_save.default.bias', 'deberta.encoder.layer.11.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.1.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.0.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.5.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.4.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.10.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.0.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.0.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.2.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.4.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.10.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.9.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.1.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.5.attention.self.value_proj.lora_B.default.weight', 'deberta.encoder.layer.7.attention.self.value_proj.lora_A.default.weight', 'classifier.original_module.bias', 'deberta.encoder.layer.11.attention.self.value_proj.lora_A.default.weight', 'deberta.encoder.layer.6.attention.self.query_proj.lora_B.default.weight', 'deberta.encoder.layer.9.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.0.attention.self.query_proj.lora_A.default.weight', 'deberta.encoder.layer.3.attention.self.value_proj.lora_B.default.weight']\n","- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at base_models/full_debertav3/full_debertav3/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForMultipleChoice.from_pretrained(f'base_models/full_debertav3/full_debertav3/',device_map={\"\": current_device})\n","model = peft.PeftModel.from_pretrained(model, \n","                                       'peft_adapters/full_debertav3/', \n","                                       is_trainable=False)\n","model.eval()\n","preds = []\n","for batch in test_dataloader:\n","    for k in batch.keys():\n","        batch[k] = batch[k].cuda()\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","    preds.append(outputs.logits.cpu().detach())\n","\n","deberta_large_preds = torch.cat(preds)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache() "]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["del model"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([200, 5]), torch.Size([200, 5]))"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["bert_large_preds.shape, deberta_large_preds.shape"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","voting_ensemble = defaultdict(list)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":[" for row in range(bert_large_preds.shape[0]):\n","        preds = bert_large_preds[row]\n","        voting_ensemble[row].append(preds.argsort(descending=True)[:3])\n","        \n"," for row in range(deberta_large_preds.shape[0]):\n","        preds = bert_large_preds[row]\n","        voting_ensemble[row].append(preds.argsort(descending=True)[:3])"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/plain":["[tensor([1, 3, 0])]"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["voting_ensemble[0][:1]"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["predictions = []\n","for row in range(bert_large_preds.shape[0]):\n","    votes = defaultdict(lambda: 0)\n","    \n","    # for preds in voting_ensemble[row][:3]: for when using 3 sets of predicions on different models\n","    #     votes[preds[0].item()] += 3\n","    #     votes[preds[1].item()] += 2\n","    #     votes[preds[2].item()] += 1\n","    \n","    bert_large_preds = voting_ensemble[row][0]\n","    votes[bert_large_preds[0].item()] += 3 * 3 # never unseat top prediction by `deberta_large_preds` even with 3,3,3 from my weights\n","    votes[bert_large_preds[1].item()] += 2 * 2 \n","    votes[bert_large_preds[2].item()] += 1 * 1 \n","    \n","    deberta_large_preds = voting_ensemble[row][1]\n","    votes[deberta_large_preds[0].item()] += 3 * 3.1 # never unseat top prediction by `deberta_large_preds` even with 3,3,3 from my weights\n","    votes[deberta_large_preds[1].item()] += 2 * 2.9 \n","    votes[deberta_large_preds[2].item()] += 1 * 2.9 \n","        \n","    predictions.append([t[0] for t in sorted(votes.items(), key=lambda x:x[1], reverse=True)][:3])"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"text/plain":["array([['B', 'D', 'A'],\n","       ['A', 'B', 'E'],\n","       ['C', 'A', 'E']], dtype='<U1')"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["predictions_as_answer_letters = np.array(list('ABCDE'))[predictions]\n","predictions_as_answer_letters[:3]"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/plain":["['B D A', 'A B E', 'C A E']"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["predictions_as_string = test_df['prediction'] = [\n","    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n","]\n","predictions_as_string[:3]"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>B D A</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>A B E</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>C A E</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>A C B</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>E C A</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id prediction\n","0   0      B D A\n","1   1      A B E\n","2   2      C A E\n","3   3      A C B\n","4   4      E C A"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["submission = test_df[['id', 'prediction']]\n","submission.to_csv('submission.csv', index=False)\n","\n","pd.read_csv('submission.csv').head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
